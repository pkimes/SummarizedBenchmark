<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SummarizedBenchmark: Introduction • SummarizedBenchmark</title>
<link rel="shortcut icon" type="image/x-icon" href="../favicon.ico">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.1/clipboard.min.js" integrity="sha256-hIvIxeqhGZF+VVeM55k0mJvWpQ6gTkWk3Emc+NmowYA=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="SummarizedBenchmark: Introduction">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">SummarizedBenchmark</a>
        <span class="label label-info" data-toggle="tooltip" data-placement="bottom" title="Bioc Devel">1.99.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/SummarizedBenchmark-Introduction.html">Quick Start</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">General</li>
    <li>
      <a href="../articles/SummarizedBenchmark-Introduction.html">Introduction</a>
    </li>
    <li>
      <a href="../articles/SummarizedBenchmark-ClassDetails.html">Class Details</a>
    </li>
    <li>
      <a href="../articles/SummarizedBenchmark-FullCaseStudy.html">Full Case Study</a>
    </li>
    <li class="divider">
    <li class="dropdown-header">Features</li>
    <li>
      <a href="../articles/Feature-ErrorHandling.html">Error Handling</a>
    </li>
    <li>
      <a href="../articles/Feature-Iterative.html">Iterative Benchmarking</a>
    </li>
    <li>
      <a href="../articles/Feature-Parallel.html">Parallelization</a>
    </li>
    <li class="divider">
    <li class="dropdown-header">More Case Studies</li>
    <li>
      <a href="../articles/CaseStudy-SingleCellSimulation.html">scRNA-seq Simulation</a>
    </li>
    <li>
      <a href="../articles/CaseStudy-RNAseqQuantification.html">RNA-seq Quantification</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/areyesq89/SummarizedBenchmark">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>SummarizedBenchmark: Introduction</h1>
                        <h4 class="author">Patrick K. Kimes, Alejandro Reyes</h4>
            
            <h4 class="date">5 November 2018</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/areyesq89/SummarizedBenchmark/blob/master/vignettes/SummarizedBenchmark-Introduction.Rmd"><code>vignettes/SummarizedBenchmark-Introduction.Rmd</code></a></small>
      <div class="hidden name"><code>SummarizedBenchmark-Introduction.Rmd</code></div>

    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      “When performing a data analysis in R, users are often presented with multiple packages and methods for accomplishing the same task. Benchmarking the performance of these different methods on real and simulated data sets is a common way of learning the relative strengths and weaknesses of each approach. However, as the number of tools and parameters increases, keeping track of output and how it was generated can quickly becomes messy. The <code>SummarizedBenchmark</code> package provides a framework for organizing benchmark comparisons, making it easier to both reproduce the original benchmark and replicate the comparison with new data. This vignette introduces the general approach and features of the package using a simple example. SummarizedBenchmark package version: 1.99.3”
    </div>
    
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>With <code>SummarizedBenchmark</code>, a complete benchmarking workflow is comprised of three primary components:</p>
<ol style="list-style-type: decimal">
<li>data,</li>
<li>methods, and</li>
<li>performance metrics.</li>
</ol>
<p>The first two (<em>data</em> and <em>methods</em>) are necessary for carrying out the benchmark experiment, and the last (<em>performance metrics</em>) is essential for evaluating the results of the experiment. Following this approach, the <code>SummarizedBenchmark</code> package defines two primary types of objects: <em>BenchDesign</em> objects and <em>SummarizedBenchmark</em> objects. <em>BenchDesign</em> objects contain only the design of the benchmark experiment, namely the <em>data</em> and <em>methods</em>, where a <em>method</em> is defined as the combination of a function or algorithm and parameter settings. After constructing a <em>BenchDesign</em>, the experiment is executed to create a <em>SummarizedBenchmark</em> containing the results of applying the methods to the data. <em>SummarizedBenchmark</em> objects extend the Bioconductor <code>SummarizedExperiment</code> class, with the additional capability of working with <em>performance metrics</em>.</p>
<p>The basic framework is illustrated in the figure below. A <em>BenchDesign</em> is created with <em>methods</em> and combined with <em>data</em> to create a <em>SummarizedBenchmark</em>, which contains the output of applying the methods to the data. This output is then paired with <em>performance metrics</em> specified by the user. Note that the same <em>BenchDesign</em> can be combined with several data sets to produce several <em>SummarizedBenchmark</em> objects with the corresponding outputs. For convenience, several default <em>performance metrics</em> are implemented in the package, and can be added to <em>SummarizedBenchmark</em> objects using simple commands.</p>
<div class="figure">
<img src="summarizedbenchmark-figure1.png" alt="basic benchmarking class relationship"><p class="caption">basic benchmarking class relationship</p>
</div>
<p>In this vignette, we first illustrate the basic use of both the <em>BenchDesign</em> and <em>SummarizedBenchmark</em> classes with a simple comparison of methods for p-value correction in the context of multiple hypothesis testing. More advanced features of the package are demonstrated in several other case study vignettes. After becoming familiar with the basic <code>SummarizedBenchmark</code> framework described in this introduction, we recommend reading the <strong>SummarizedBenchmark: Class Details</strong> vignette for more on the structure of the <em>BenchDesign</em> and <em>SummarizedBenchmark</em> classes and details on issues of <em>reproducibility</em> using these classes. Then, we recommend moving on to the more detailed <strong>SummarizedBenchmark: Full Case Study</strong> vignette where we describe more advanced features of the package with a case study comparing three methods for differential expression analysis.</p>
<div id="related-work" class="section level2">
<h2 class="hasAnchor">
<a href="#related-work" class="anchor"></a>Related Work</h2>
<p>Other frameworks for benchmarking methods include <em><a href="http://bioconductor.org/packages/iCOBRA">iCOBRA</a></em> (a package for comparing results of <em>“binary classification and ranking methods”</em> with a Shiny web application for interactive analyses), <em><a href="http://bioconductor.org/packages/rnaseqcomp">rnaseqcomp</a></em> (a package for comparing results of RNA-seq quantification pipelines), and <em><a href="https://github.com/stephenslab/dsc">dsc</a></em> (a framework for <em>“managing computational benchmarking experiments that compare several competing methods”</em> written in Python but capable of running methods implemented in both Python and R).</p>
</div>
</div>
<div id="quickstart-case-study" class="section level1">
<h1 class="hasAnchor">
<a href="#quickstart-case-study" class="anchor"></a>Quickstart Case Study</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(<span class="st">"SummarizedBenchmark"</span>)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(<span class="st">"magrittr"</span>)</a></code></pre></div>
<p>To illustrate the basic use of the <em>BenchDesign</em> class, we use the <code>tdat</code> data set included with this package.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">data</span>(tdat)</a></code></pre></div>
<p>The data set is a <em>data.frame</em> containing the results of 50 two-sample t-tests. The tests were performed using independently simulated sets of 20 observations drawn from a single standard Normal distribution (when <code>H = 0</code>) or two mean-shifted Normal distributions (when <code>H = 1</code>).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">head</span>(tdat)</a></code></pre></div>
<pre><code>##   H test_statistic effect_size         pval        SE
## 1 1     -3.2083437 -1.17151466 4.872282e-03 0.3651463
## 2 0      0.1692236  0.07321912 8.675080e-01 0.4326768
## 3 1     -5.7077940 -1.81715381 2.061521e-05 0.3183636
## 4 1     -1.9805856 -1.09107836 6.313031e-02 0.5508867
## 5 1     -1.0014358 -0.37726058 3.298895e-01 0.3767197
## 6 1     -0.9190433 -0.47583669 3.702252e-01 0.5177522</code></pre>
<p>Several approaches have been proposed and implemented to compute <em>adjusted p-values</em> and <em>q-values</em> with the goal of controlling the total number of false discoveries across a collection of tests. In this example, we compare three such methods:</p>
<ol style="list-style-type: decimal">
<li>Bonferroni correction (<code>p.adjust</code> w/ <code>method = "bonferroni"</code>) <span class="citation">(Dunn 1961)</span>,</li>
<li>Benjamini-Hochberg (<code>p.adjust</code> w/ <code>method = "BH"</code>) <span class="citation">(Benjamini and Hochberg 1995)</span>, and</li>
<li>Storey’s FDR q-value (<code><a href="http://www.rdocumentation.org/packages/qvalue/topics/qvalue">qvalue::qvalue</a></code>) <span class="citation">(Storey 2002)</span>.</li>
</ol>
<p>First, consider how benchmarking the three methods might look without the <em>SummarizedBenchmark</em> framework.</p>
<p>To compare methods, each is applied to <code>tdat</code>, and the results are stored in separate variables.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">adj_bonf &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(<span class="dt">p =</span> tdat<span class="op">$</span>pval, <span class="dt">method =</span> <span class="st">"bonferroni"</span>)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">adj_bh &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(<span class="dt">p =</span> tdat<span class="op">$</span>pval, <span class="dt">method =</span> <span class="st">"BH"</span>)</a>
<a class="sourceLine" id="cb5-4" data-line-number="4"></a>
<a class="sourceLine" id="cb5-5" data-line-number="5">qv &lt;-<span class="st"> </span>qvalue<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/qvalue/topics/qvalue">qvalue</a></span>(<span class="dt">p =</span> tdat<span class="op">$</span>pval)</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">adj_qv &lt;-<span class="st"> </span>qv<span class="op">$</span>qvalues</a></code></pre></div>
<p>Since the values of interest are available from the ouput of each method as a vector of length 50 (the number of hypotheses tested), to keep things clean, they can be combined into a single <em>data.frame</em>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">adj &lt;-<span class="st"> </span><span class="kw">cbind.data.frame</span>(adj_bonf, adj_bh, adj_qv)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"><span class="kw">head</span>(adj)</a></code></pre></div>
<pre><code>##     adj_bonf     adj_bh       adj_qv
## 1 0.24361409 0.02191765 0.0079813228
## 2 1.00000000 0.90365415 0.3290660527
## 3 0.00103076 0.00103076 0.0003753518
## 4 1.00000000 0.12140444 0.0442094809
## 5 1.00000000 0.47127065 0.1716134119
## 6 1.00000000 0.48250104 0.1757029652</code></pre>
<p>The <em>data.frame</em> of adjusted p-values and q-values can be used to compare the methods, either by directly parsing the table or using a framework like <em><a href="http://bioconductor.org/packages/iCOBRA">iCOBRA</a></em>. Additionally, the <em>data.frame</em> can be saved as a <code>RDS</code> or <code>Rdata</code> object for future reference, eliminating the need for recomputing on the original data.</p>
<p>While this approach can work well for smaller comparisons, it can quickly become overwhelming and unweildy as the number of methods and parameters increases. Furthermore, once each method is applied and the final <em>data.frame</em> (<code>adj</code>) is constructed, there is no way to determine <em>how</em> each value was calculated. While an informative name can be used to “label” each method (as done above), this does not capture the full complexity, e.g. parameters and context, where the function was evaluated. One solution might involve manually recording function calls and parameters in a separate <em>data.frame</em> with the hope of maintaining synchrony with the output <em>data.frame</em>. However, this is prone to errors, e.g. during fast “copy and paste” operations or additions and deletions of parameter combinations. An alternative (and hopefully better) solution, is to use the framework of the <em>SummarizedBenchmark</em> package.</p>
<p>In the <em>SummarizedBenchmark</em> approach, a <em>BenchDesign</em> is constructed with the data and any number of methods. Optionally, a <em>BenchDesign</em> can also be constructed without any data or method inputs. Methods and data can be added or removed from the object modularly in a few different ways, as will be described in the following section. For simplicity, we first show how to construct the <em>BenchDesign</em> with just the data set as input. The data object, here <code>tdat</code>, must be passed explicitly to the <code>data =</code> parameter.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">b &lt;-<span class="st"> </span><span class="kw"><a href="../reference/BenchDesign.html">BenchDesign</a></span>(<span class="dt">data =</span> tdat)</a></code></pre></div>
<p>Then, each method of interest can be added to the <em>BenchDesign</em> using <code><a href="../reference/addMethod.html">addMethod()</a></code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">b &lt;-<span class="st"> </span><span class="kw"><a href="../reference/addMethod.html">addMethod</a></span>(<span class="dt">bd =</span> b, <span class="dt">label =</span> <span class="st">"bonf"</span>, <span class="dt">func =</span> p.adjust,</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">               <span class="dt">params =</span> rlang<span class="op">::</span><span class="kw"><a href="https://rlang.r-lib.org/reference/quotation.html">quos</a></span>(<span class="dt">p =</span> pval, <span class="dt">method =</span> <span class="st">"bonferroni"</span>))</a></code></pre></div>
<p>At a minimum, <code><a href="../reference/addMethod.html">addMethod()</a></code> requires three parameters:</p>
<ol style="list-style-type: decimal">
<li>
<code>bd</code>: the <em>BenchDesign</em> object to modify,</li>
<li>
<code>label</code>: a character name for the method, and</li>
<li>
<code>func</code>: the function to be called.</li>
</ol>
<p>After the minimum parameters are specified, any parameters needed by the <code>func</code> method should be passed as named parameters, e.g. <code>p = pval, method = "bonferroni"</code>, to <code>params =</code> as a list of <a href="http://rlang.tidyverse.org/reference/quosure.html"><em>quosures</em></a> using <code><a href="https://rlang.r-lib.org/reference/quotation.html">rlang::quos(..)</a></code>. Notice here that the <code>pval</code> wrapped in <code><a href="https://rlang.r-lib.org/reference/quotation.html">rlang::quos(..)</a></code> <strong>does not</strong> need to be specified as <code>tdat$pval</code> for the function to access the column in the data.frame. For readers familiar with the <em><a href="https://CRAN.R-project.org/package=ggplot2">ggplot2</a></em> package, the use of <code>params = rlang::quos(..)</code> here should be viewed similar to the use of <code>aes = aes(..)</code> in <code>ggplot2</code> for mapping between the data and plotting (or benchmarking) parameters.</p>
<p>The process of adding methods can be written more concisely using the pipe operators from the <em><a href="https://CRAN.R-project.org/package=magrittr">magrittr</a></em> package.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1">b &lt;-<span class="st"> </span>b <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="st">    </span><span class="kw"><a href="../reference/addMethod.html">addMethod</a></span>(<span class="dt">label =</span> <span class="st">"BH"</span>,</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">              <span class="dt">func =</span> p.adjust,</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">              <span class="dt">params =</span> rlang<span class="op">::</span><span class="kw"><a href="https://rlang.r-lib.org/reference/quotation.html">quos</a></span>(<span class="dt">p =</span> pval, <span class="dt">method =</span> <span class="st">"BH"</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb10-5" data-line-number="5"><span class="st">    </span><span class="kw"><a href="../reference/addMethod.html">addMethod</a></span>(<span class="dt">label =</span> <span class="st">"qv"</span>,</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">              <span class="dt">func =</span> qvalue<span class="op">::</span>qvalue,</a>
<a class="sourceLine" id="cb10-7" data-line-number="7">              <span class="dt">params =</span> rlang<span class="op">::</span><span class="kw"><a href="https://rlang.r-lib.org/reference/quotation.html">quos</a></span>(<span class="dt">p =</span> pval),</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">              <span class="dt">post =</span> <span class="cf">function</span>(x) { x<span class="op">$</span>qvalues })</a></code></pre></div>
<p>For some methods, such as the q-value approach above, it may be necessary to call a “post-processing” function on the primary method to extract the desired output (here, the q-values). This should be specified using the optional <code>post =</code> parameter.</p>
<p>Now, the <em>BenchDesign</em> object contains three methods. This can be verified either by calling on the object.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">b</a></code></pre></div>
<pre><code>## BenchDesign ------------------------------------------------ 
##   benchmark data:
##     type: data 
##     names: H, test_statistic, effect_size, pval, SE 
##   benchmark methods:
##     method:  bonf; func: p.adjust       
##     method:    BH; func: p.adjust       
##     method:    qv; func: qvalue::qvalue</code></pre>
<p>More details about each method can be seen by using the <code><a href="../reference/printMethod.html">printMethods()</a></code> function.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw"><a href="../reference/printMethod.html">printMethods</a></span>(b)</a></code></pre></div>
<pre><code>## bonf ------------------------------------------------------- 
## BenchDesign Method (BDMethod) ------------------------------ 
##   method: p.adjust
##     function (p, method = p.adjust.methods, n = length(p)... 
##   parameters:
##     p : pval 
##     method : "bonferroni" 
##   post:
##     none
##   meta:
##     none
## BH --------------------------------------------------------- 
## BenchDesign Method (BDMethod) ------------------------------ 
##   method: p.adjust
##     function (p, method = p.adjust.methods, n = length(p)... 
##   parameters:
##     p : pval 
##     method : "BH" 
##   post:
##     none
##   meta:
##     none
## qv --------------------------------------------------------- 
## BenchDesign Method (BDMethod) ------------------------------ 
##   method: qvalue::qvalue
##     function (p, fdr.level = NULL, pfdr = FALSE, lfdr.out... 
##   parameters:
##     p : pval 
##   post:
##     default : function (x) { ... 
##   meta:
##     none</code></pre>
<p>While the bench now includes all the information necessary for performing the benchmarking study, the actual adjusted p-values and q-values have not yet been calculated. To do this, we simply call <code><a href="../reference/buildBench.html">buildBench()</a></code>. While <code><a href="../reference/buildBench.html">buildBench()</a></code> does not require any inputs other than the <em>BenchDesign</em> object, when the corresponding ground truth is known, the <code>truthCols =</code> parameter should be specified. In this example, the <code>H</code> column of the <code>tdat</code> <em>data.frame</em> contains the true null or alternative status of each simulated hypothesis test. Note that if any of the methods are defined in a separate package, they must be installed and loaded <em>before</em> running the experiment.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">sb &lt;-<span class="st"> </span><span class="kw"><a href="../reference/buildBench.html">buildBench</a></span>(b, <span class="dt">truthCols =</span> <span class="st">"H"</span>)</a></code></pre></div>
<p>The returned object is a <em>SummarizedBenchmark</em> class. The <em>SummarizedBenchmark</em> object is an extension of a <em>SummarizedExperiment</em> object. The table of adjusted p-values and q-values is contained in a single “assay” of the object with each method added using <code><a href="../reference/addMethod.html">addMethod()</a></code> as a column with the corresponding <code>label</code> as the name.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">assay</span>(sb))</a></code></pre></div>
<pre><code>##            bonf         BH           qv
## [1,] 0.24361409 0.02191765 0.0079813228
## [2,] 1.00000000 0.90365415 0.3290660527
## [3,] 0.00103076 0.00103076 0.0003753518
## [4,] 1.00000000 0.12140444 0.0442094809
## [5,] 1.00000000 0.47127065 0.1716134119
## [6,] 1.00000000 0.48250104 0.1757029652</code></pre>
<p>Metadata for the methods is contained in the <code>colData()</code> of the same object, with each row corresponding to one method in the comparison.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">colData</span>(sb)</a></code></pre></div>
<pre><code>## DataFrame with 3 rows and 6 columns
##         func.pkg func.pkg.vers func.pkg.manual     param.p param.method
##      &lt;character&gt;   &lt;character&gt;       &lt;logical&gt; &lt;character&gt;  &lt;character&gt;
## bonf       stats         3.5.0           FALSE        pval "bonferroni"
## BH         stats         3.5.0           FALSE        pval         "BH"
## qv        qvalue        2.12.0           FALSE        pval           NA
##      session.idx
##        &lt;numeric&gt;
## bonf           1
## BH             1
## qv             1</code></pre>
<p>In addition to columns for the functions and parameters specified with <code><a href="../reference/addMethod.html">addMethod()</a></code> (<code>func, post, label, param.*</code>), the <code>colData()</code> includes several other columns added during the <code><a href="../reference/buildBench.html">buildBench()</a></code> process. Most notably, columns for the package name and version of <code>func</code> if available (<code>func.pkg</code>, <code>func.pkg.vers</code>).</p>
<p>When available, ground truth data is contained in the <code>rowData()</code> of the <em>SummarizedBenchmark</em> object.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">rowData</span>(sb)</a></code></pre></div>
<pre><code>## DataFrame with 50 rows and 1 column
##             H
##     &lt;numeric&gt;
## 1           1
## 2           0
## 3           1
## 4           1
## 5           1
## ...       ...
## 46          0
## 47          1
## 48          0
## 49          1
## 50          1</code></pre>
<p>In addition, the <em>SummarizedBenchmark</em> class contains an additional slot where users can define performance metrics to evaluate the different methods. Since different benchmarking experiments may require the use of different metrics to evaluate the performance of the methods, the <em>SummarizedBenchmark</em> class provides a flexible way to define performance metrics. We can define performance metrics using the function <code><a href="../reference/addPerformanceMetric.html">addPerformanceMetric()</a></code> by providing a <em>SummarizedBenchmark</em> object, a name of the metric, an assay name, and the function that defines it. Importantly, the function must contain the following two arguments: query (referring to a vector of values being evaluated, i.e. the output of one method) and truth (referring to the vector of ground truths). If further arguments are provided to the performance function, these must contain default values.</p>
<p>For our example, we define the performance metric “TPR” (True Positive Rate) that calculates the fraction of true positives recovered given an alpha value. This performance metric uses the <code>H</code> assay of our <em>SummarizedBenchmark</em> example object.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">sb &lt;-<span class="st"> </span><span class="kw"><a href="../reference/addPerformanceMetric.html">addPerformanceMetric</a></span>(</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">  <span class="dt">object =</span> sb,</a>
<a class="sourceLine" id="cb22-3" data-line-number="3">  <span class="dt">assay =</span> <span class="st">"H"</span>,</a>
<a class="sourceLine" id="cb22-4" data-line-number="4">  <span class="dt">evalMetric =</span> <span class="st">"TPR"</span>,</a>
<a class="sourceLine" id="cb22-5" data-line-number="5">  <span class="dt">evalFunction =</span> <span class="cf">function</span>(query, truth, <span class="dt">alpha =</span> <span class="fl">0.1</span>) {</a>
<a class="sourceLine" id="cb22-6" data-line-number="6">    goodHits &lt;-<span class="st"> </span><span class="kw">sum</span>((query <span class="op">&lt;</span><span class="st"> </span>alpha) <span class="op">&amp;</span><span class="st"> </span>truth <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb22-7" data-line-number="7">    goodHits <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(truth <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb22-8" data-line-number="8">    }</a>
<a class="sourceLine" id="cb22-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb22-10" data-line-number="10"></a>
<a class="sourceLine" id="cb22-11" data-line-number="11"><span class="kw"><a href="../reference/performanceMetrics.html">performanceMetrics</a></span>(sb)[[<span class="st">"H"</span>]]</a></code></pre></div>
<pre><code>## $TPR
## function (query, truth, alpha = 0.1) 
## {
##     goodHits &lt;- sum((query &lt; alpha) &amp; truth == 1)
##     goodHits/sum(truth == 1)
## }</code></pre>
<p>Having defined all the desired performance metrics, the function <code><a href="../reference/estimateMetrics.html">estimatePerformanceMetrics()</a></code> calculates these for each method. Parameters for the performance functions can be passed here. In the case below, we specify several <code>alpha =</code> values to be used for calculating the performance metrics with each function.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">resWide &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estimateMetrics.html">estimatePerformanceMetrics</a></span>(sb, <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>))</a>
<a class="sourceLine" id="cb24-2" data-line-number="2">resWide</a></code></pre></div>
<pre><code>## DataFrame with 3 rows and 9 columns
##         func.pkg func.pkg.vers func.pkg.manual     param.p param.method
##      &lt;character&gt;   &lt;character&gt;       &lt;logical&gt; &lt;character&gt;  &lt;character&gt;
## bonf       stats         3.5.0           FALSE        pval "bonferroni"
## BH         stats         3.5.0           FALSE        pval         "BH"
## qv        qvalue        2.12.0           FALSE        pval           NA
##      session.idx     TPR.1     TPR.2     TPR.3
##        &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;
## bonf           1     0.125       0.2     0.225
## BH             1     0.375      0.55     0.625
## qv             1       0.6       0.7       0.9</code></pre>
<p>By default, the function above returns a <em>DataFrame</em>, where the parameters of the performance function are stored in its <code>elementMetadata()</code>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">elementMetadata</span>(resWide)</a></code></pre></div>
<pre><code>## DataFrame with 9 rows and 4 columns
##             colType       assay performanceMetric     alpha
##         &lt;character&gt; &lt;character&gt;       &lt;character&gt; &lt;numeric&gt;
## 1 methodInformation          NA                NA        NA
## 2 methodInformation          NA                NA        NA
## 3 methodInformation          NA                NA        NA
## 4 methodInformation          NA                NA        NA
## 5 methodInformation          NA                NA        NA
## 6 methodInformation          NA                NA        NA
## 7 performanceMetric           H               TPR      0.05
## 8 performanceMetric           H               TPR       0.1
## 9 performanceMetric           H               TPR       0.2</code></pre>
<p>A second possibility is to set the parameter <code>addColData = TRUE</code> for these results to be stored in the <code>colData()</code> of the <em>SummarizedBenchmark</em> object.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">sb &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estimateMetrics.html">estimatePerformanceMetrics</a></span>(sb, </a>
<a class="sourceLine" id="cb28-2" data-line-number="2">                                 <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>), </a>
<a class="sourceLine" id="cb28-3" data-line-number="3">                                 <span class="dt">addColData =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb28-4" data-line-number="4"><span class="kw">colData</span>(sb)</a></code></pre></div>
<pre><code>## DataFrame with 3 rows and 10 columns
##         func.pkg func.pkg.vers func.pkg.manual     param.p param.method
##      &lt;character&gt;   &lt;character&gt;       &lt;logical&gt; &lt;character&gt;  &lt;character&gt;
## bonf       stats         3.5.0           FALSE        pval "bonferroni"
## BH         stats         3.5.0           FALSE        pval         "BH"
## qv        qvalue        2.12.0           FALSE        pval           NA
##      session.idx     TPR.1     TPR.2     TPR.3 pm.session
##        &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt;
## bonf           1     0.125       0.2     0.225          1
## BH             1     0.375      0.55     0.625          1
## qv             1       0.6       0.7       0.9          1</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="kw">elementMetadata</span>(<span class="kw">colData</span>(sb))</a></code></pre></div>
<pre><code>## DataFrame with 10 rows and 4 columns
##                     colType       assay performanceMetric     alpha
##                 &lt;character&gt; &lt;character&gt;       &lt;character&gt; &lt;numeric&gt;
## 1         methodInformation          NA                NA        NA
## 2         methodInformation          NA                NA        NA
## 3         methodInformation          NA                NA        NA
## 4         methodInformation          NA                NA        NA
## 5         methodInformation          NA                NA        NA
## 6         methodInformation          NA                NA        NA
## 7         performanceMetric           H               TPR      0.05
## 8         performanceMetric           H               TPR       0.1
## 9         performanceMetric           H               TPR       0.2
## 10 performanceMetricSession          NA                NA        NA</code></pre>
<p>Finally, if the user prefers tidier formats, by setting the parameter <code>tidy = TRUE</code> the function returns a long-formated version of the results.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1"><span class="kw"><a href="../reference/estimateMetrics.html">estimatePerformanceMetrics</a></span>(sb, </a>
<a class="sourceLine" id="cb32-2" data-line-number="2">                           <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>), </a>
<a class="sourceLine" id="cb32-3" data-line-number="3">                           <span class="dt">tidy =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##   func.pkg func.pkg.vers func.pkg.manual param.p param.method session.idx
## 1    stats         3.5.0           FALSE    pval "bonferroni"           1
## 2    stats         3.5.0           FALSE    pval         "BH"           1
## 3   qvalue        2.12.0           FALSE    pval         &lt;NA&gt;           1
## 4    stats         3.5.0           FALSE    pval "bonferroni"           1
## 5    stats         3.5.0           FALSE    pval         "BH"           1
## 6   qvalue        2.12.0           FALSE    pval         &lt;NA&gt;           1
## 7    stats         3.5.0           FALSE    pval "bonferroni"           1
## 8    stats         3.5.0           FALSE    pval         "BH"           1
## 9   qvalue        2.12.0           FALSE    pval         &lt;NA&gt;           1
##   pm.session label   key value assay performanceMetric alpha
## 1          1  bonf TPR.1 0.125     H               TPR  0.05
## 2          1    BH TPR.1 0.375     H               TPR  0.05
## 3          1    qv TPR.1 0.600     H               TPR  0.05
## 4          1  bonf TPR.2 0.200     H               TPR  0.10
## 5          1    BH TPR.2 0.550     H               TPR  0.10
## 6          1    qv TPR.2 0.700     H               TPR  0.10
## 7          1  bonf TPR.3 0.225     H               TPR  0.20
## 8          1    BH TPR.3 0.625     H               TPR  0.20
## 9          1    qv TPR.3 0.900     H               TPR  0.20</code></pre>
<p>As an alternative to get the same <em>data.frame</em> as the previous chunk, we can call the function <code><a href="../reference/tidyUpMetrics.html">tidyUpMetrics()</a></code> on the saved results from a <em>SummarizedBenchmark</em> object.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="kw">head</span>(<span class="kw"><a href="../reference/tidyUpMetrics.html">tidyUpMetrics</a></span>(sb))</a></code></pre></div>
<pre><code>##   func.pkg func.pkg.vers func.pkg.manual param.p param.method session.idx
## 1    stats         3.5.0           FALSE    pval "bonferroni"           1
## 2    stats         3.5.0           FALSE    pval         "BH"           1
## 3   qvalue        2.12.0           FALSE    pval         &lt;NA&gt;           1
## 4    stats         3.5.0           FALSE    pval "bonferroni"           1
## 5    stats         3.5.0           FALSE    pval         "BH"           1
## 6   qvalue        2.12.0           FALSE    pval         &lt;NA&gt;           1
##   pm.session label   key value assay performanceMetric alpha
## 1          1  bonf TPR.1 0.125     H               TPR  0.05
## 2          1    BH TPR.1 0.375     H               TPR  0.05
## 3          1    qv TPR.1 0.600     H               TPR  0.05
## 4          1  bonf TPR.2 0.200     H               TPR  0.10
## 5          1    BH TPR.2 0.550     H               TPR  0.10
## 6          1    qv TPR.2 0.700     H               TPR  0.10</code></pre>
<p>For example, the code below extracts the TPR for an alpha of 0.1 for the Bonferroni method.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1"><span class="kw"><a href="../reference/tidyUpMetrics.html">tidyUpMetrics</a></span>(sb) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="st">  </span>dplyr<span class="op">:::</span><span class="kw">filter</span>(label <span class="op">==</span><span class="st"> "bonf"</span>, alpha <span class="op">==</span><span class="st"> </span><span class="fl">0.1</span>, performanceMetric <span class="op">==</span><span class="st"> "TPR"</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb36-3" data-line-number="3"><span class="st">  </span>dplyr<span class="op">:::</span><span class="kw">select</span>(value)</a></code></pre></div>
<pre><code>##   value
## 1   0.2</code></pre>
</div>
<div id="next-steps" class="section level1">
<h1 class="hasAnchor">
<a href="#next-steps" class="anchor"></a>Next Steps</h1>
<p>This vignette described the minimal structure of the <code>SummarizedBenchmark</code> framework using the <em>BenchDesign</em> and <em>SummarizedBenchmark</em> classes. This should be sufficient for building and executing simple benchmarks but may not be enough for more complex benchmarking experiments. A more detailed example using the <code>SummarizedBenchmark</code> is provided in the <strong>SummarizedBenchmark: Full Case Study</strong> vignette and additional features can be found the various <strong>Feature</strong> vignettes. A more complete description of the <em>BenchDesign</em> and <em>SummarizedBenchmark</em> classes can be found in the <strong>SummarizedBenchmark: Class Details</strong> vignette.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-Benjamini_1995">
<p>Benjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 289–300.</p>
</div>
<div id="ref-Dunn_1961">
<p>Dunn, Olive Jean. 1961. “Multiple Comparisons Among Means.” <em>Journal of the American Statistical Association</em> 56 (293):52–64.</p>
</div>
<div id="ref-Storey_2002">
<p>Storey, John D. 2002. “A Direct Approach to False Discovery Rates.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 64 (3):479–98.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#quickstart-case-study">Quickstart Case Study</a></li>
      <li><a href="#next-steps">Next Steps</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by <a href="http://alejandroreyes.org">Alejandro Reyes</a>, <a href="https://www.pkimes.com">Patrick Kimes</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
